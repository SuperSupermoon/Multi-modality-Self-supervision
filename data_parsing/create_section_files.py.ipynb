{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance\n",
    "import math\n",
    "import random\n",
    "import tqdm as tqdm\n",
    "import albumentations\n",
    "import collections\n",
    "import re\n",
    "# This script extracts the conclusion section from MIMIC-CXR reports\n",
    "# It outputs them into individual files with at most 10,000 reports.\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# local folder import\n",
    "import section_parser as sp\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of jpg files 313665\n",
      "# of free_text files 227835\n"
     ]
    }
   ],
   "source": [
    "# pd.set_option('display.max_colwidth', -1)\n",
    "jpg = glob(\"/home/ubuntu/mimic_jpg/physionet.org/files/mimic-cxr-jpg/2.0.0/files/*/*/*/*.jpg\")\n",
    "free_text = glob(\"/home/ubuntu/mimic_jpg/files/*/*/*.txt\")\n",
    "\n",
    "jpg = pd.DataFrame({\"dir\" : jpg})\n",
    "free_text = pd.DataFrame({\"dir\" : free_text})\n",
    "\n",
    "print(\"# of jpg files\", len(jpg))\n",
    "print(\"# of free_text files\",len(free_text))\n",
    "#path matching\n",
    "jpg[\"study\"] = jpg[\"dir\"].str.split(pat = \"/\", expand = True)[11]\n",
    "jpg[\"num_of_study\"] = pd.to_numeric(jpg.study.str.slice(start=1))\n",
    "\n",
    "free_text[\"study\"] = free_text[\"dir\"].str.split(pat = \"/\", expand = True)[7]\n",
    "a = free_text[\"study\"].str.split(pat = \".\", expand = True)\n",
    "free_text[\"num_of_study\"] = pd.to_numeric(a[0].str.slice(start=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/mimic_jpg/physionet.org/files/mimic-cxr-jpg/2.0.0/files/p11/p11573679/s58235663/1a671a62-0a32dfc6-5f85029c-81c3922e-3f5a2c27.jpg',\n",
       " '/home/ubuntu/mimic_jpg/physionet.org/files/mimic-cxr-jpg/2.0.0/files/p16/p16175671/s56724958/92c2d3f2-81af6356-c4ba9b42-1c9636b9-ae69d8b3.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk_txt_2 = glob(\"/home/ubuntu/check_txt/2manual_parsing/*.txt\")\n",
    "chk_txt_24 = glob(\"/home/ubuntu/check_txt/24manual_parsing/*.txt\")\n",
    "\n",
    "chk_txt_2 = pd.DataFrame({\"dir\" : chk_txt_2})\n",
    "chk_txt_24 = pd.DataFrame({\"dir\" : chk_txt_24})\n",
    "\n",
    "chk_txt_24[\"study\"] = chk_txt_24[\"dir\"].str.split(pat = \"/\", expand = True)[5]\n",
    "re_split = chk_txt_24[\"study\"].str.split(pat = \".\", expand = True)\n",
    "chk_txt_24[\"study number\"] = pd.to_numeric(re_split[0].str.slice(start=1))\n",
    "\n",
    "# chk_txt_path = jpg[jpg.num_of_study.isin(chk_txt_24['study number'])]\n",
    "# chk_txt_path_list = chk_txt_path.dir.tolist() \n",
    "\n",
    "# chk_txt_path_list2\n",
    "\n",
    "chk_txt_2[\"study\"] = chk_txt_2[\"dir\"].str.split(pat = \"/\", expand = True)[5]\n",
    "re_split2 = chk_txt_2[\"study\"].str.split(pat = \".\", expand = True)\n",
    "chk_txt_2[\"study number\"] = pd.to_numeric(re_split2[0].str.slice(start=1))\n",
    "\n",
    "chk_txt_path2 = jpg[jpg.num_of_study.isin(chk_txt_2['study number'])]\n",
    "chk_txt_path_list2 = chk_txt_path2.dir.tolist() \n",
    "\n",
    "chk_txt_path_list2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--reports_path',\n",
    "                    required=True,\n",
    "                    help=('Path to file with radiology reports,'\n",
    "                          ' e.g. /data/mimic-cxr/files'))\n",
    "parser.add_argument('--output_path',\n",
    "                    required=True,\n",
    "                    help='Path to output CSV files.')\n",
    "parser.add_argument('--no_split', action='store_true',\n",
    "                    help='Do not output batched CSV files.')\n",
    "\n",
    "\n",
    "def list_rindex(l, s):\n",
    "    \"\"\"Helper function: *last* matching element in a list\"\"\"\n",
    "    return len(l) - l[-1::-1].index(s) - 1\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    args = parser.parse_args(args)\n",
    "\n",
    "    reports_path = Path(args.reports_path)\n",
    "    output_path = Path(args.output_path)\n",
    "\n",
    "    if not output_path.exists():\n",
    "        output_path.mkdir()\n",
    "\n",
    "    # not all reports can be automatically sectioned\n",
    "    # we load in some dictionaries which have manually determined sections\n",
    "    custom_section_names, custom_indices = sp.custom_mimic_cxr_rules()\n",
    "\n",
    "    # get all higher up folders (p00, p01, etc)\n",
    "    p_grp_folders = os.listdir(reports_path)\n",
    "    p_grp_folders = [p for p in p_grp_folders\n",
    "                     if p.startswith('p') and len(p) == 3]\n",
    "    p_grp_folders.sort()\n",
    "\n",
    "    # patient_studies will hold the text for use in NLP labeling\n",
    "    patient_studies = []\n",
    "\n",
    "    # study_sections will have an element for each study\n",
    "    # this element will be a list, each element having text for a specific section\n",
    "    study_sections = []\n",
    "    for p_grp in p_grp_folders:\n",
    "        # get patient folders, usually around ~6k per group folder\n",
    "        cxr_path = reports_path / p_grp\n",
    "        p_folders = os.listdir(cxr_path)\n",
    "        p_folders = [p for p in p_folders if p.startswith('p')]\n",
    "        p_folders.sort()\n",
    "\n",
    "        # For each patient in this grouping folder\n",
    "        print(p_grp)\n",
    "        for p in tqdm(p_folders):\n",
    "            patient_path = cxr_path / p\n",
    "\n",
    "            # get the filename for all their free-text reports\n",
    "            studies = os.listdir(patient_path)\n",
    "            studies = [s for s in studies\n",
    "                       if s.endswith('.txt') and s.startswith('s')]\n",
    "\n",
    "            for s in studies:\n",
    "                # load in the free-text report\n",
    "                with open(patient_path / s, 'r') as fp:\n",
    "                    text = ''.join(fp.readlines())\n",
    "\n",
    "                # get study string name without the txt extension\n",
    "                s_stem = s[0:-4]\n",
    "\n",
    "                # custom rules for some poorly formatted reports\n",
    "                if s_stem in custom_indices:\n",
    "                    idx = custom_indices[s_stem]\n",
    "                    patient_studies.append([s_stem, text[idx[0]:idx[1]]])\n",
    "                    continue\n",
    "\n",
    "                # split text into sections\n",
    "                sections, section_names, section_idx = sp.section_text(\n",
    "                    text\n",
    "                )\n",
    "\n",
    "                # check to see if this has mis-named sections\n",
    "                # e.g. sometimes the impression is in the comparison section\n",
    "                if s_stem in custom_section_names:\n",
    "                    sn = custom_section_names[s_stem]\n",
    "                    idx = list_rindex(section_names, sn)\n",
    "                    patient_studies.append([s_stem, sections[idx].strip()])\n",
    "                    continue\n",
    "\n",
    "                # grab the *last* section with the given title\n",
    "                # prioritizes impression > findings, etc.\n",
    "\n",
    "                # \"last_paragraph\" is text up to the end of the report\n",
    "                # many reports are simple, and have a single section\n",
    "                # header followed by a few paragraphs\n",
    "                # these paragraphs are grouped into section \"last_paragraph\"\n",
    "\n",
    "                # note also comparison seems unusual but if no other sections\n",
    "                # exist the radiologist has usually written the report\n",
    "                # in the comparison section\n",
    "                idx = -1\n",
    "                for sn in ('impression', 'findings',\n",
    "                           'last_paragraph', 'comparison'):\n",
    "                    if sn in section_names:\n",
    "                        idx = list_rindex(section_names, sn)\n",
    "                        break\n",
    "\n",
    "                if idx == -1:\n",
    "                    # we didn't find any sections we can use :(\n",
    "                    patient_studies.append([s_stem, ''])\n",
    "                    print(f'no impression/findings: {patient_path / s}')\n",
    "                else:\n",
    "                    # store the text of the conclusion section\n",
    "                    patient_studies.append([s_stem, sections[idx].strip()])\n",
    "\n",
    "                study_sectioned = [s_stem]\n",
    "                for sn in ('impression', 'findings',\n",
    "                           'last_paragraph', 'comparison'):\n",
    "                    if sn in section_names:\n",
    "                        idx = list_rindex(section_names, sn)\n",
    "                        study_sectioned.append(sections[idx].strip())\n",
    "                    else:\n",
    "                        study_sectioned.append(None)\n",
    "                study_sections.append(study_sectioned)\n",
    "    # write distinct files to facilitate modular processing\n",
    "    if len(patient_studies) > 0:\n",
    "        # write out a single CSV with the sections\n",
    "        with open(output_path / 'mimic_cxr_sectioned.csv', 'w') as fp:\n",
    "            csvwriter = csv.writer(fp)\n",
    "            # write header\n",
    "            csvwriter.writerow(['study', 'impression', 'findings',\n",
    "                                'last_paragraph', 'comparison'])\n",
    "            for row in study_sections:\n",
    "                csvwriter.writerow(row)\n",
    "\n",
    "        if args.no_split:\n",
    "            # write all the reports out to a single file\n",
    "            with open(output_path / f'mimic_cxr_sections.csv', 'w') as fp:\n",
    "                csvwriter = csv.writer(fp)\n",
    "                for row in patient_studies:\n",
    "                    csvwriter.writerow(row)\n",
    "        else:\n",
    "            # write ~22 files with ~10k reports each\n",
    "            n = 0\n",
    "            jmp = 10000\n",
    "\n",
    "            while n < len(patient_studies):\n",
    "                n_fn = n // jmp\n",
    "                with open(output_path / f'mimic_cxr_{n_fn:02d}.csv', 'w') as fp:\n",
    "                    csvwriter = csv.writer(fp)\n",
    "                    for row in patient_studies[n:n+jmp]:\n",
    "                        csvwriter.writerow(row)\n",
    "                n += jmp\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_mimic_cxr_rules():\n",
    "    custom_section_names = {\n",
    "        's50913680': 'recommendations',  # files/p11/p11851243/s50913680.txt\n",
    "        's59363654': 'examination',  # files/p12/p12128253/s59363654.txt\n",
    "        's59279892': 'technique',  # files/p13/p13150370/s59279892.txt\n",
    "        's59768032': 'recommendations',  # files/p13/p13249077/s59768032.txt\n",
    "        's57936451': 'indication',  # files/p14/p14325424/s57936451.txt\n",
    "        's50058765': 'indication',  # files/p14/p14731346/s50058765.txt\n",
    "        's53356173': 'examination',  # files/p15/p15898350/s53356173.txt\n",
    "        's53202765': 'technique',  # files/p16/p16076182/s53202765.txt\n",
    "        's50808053': 'technique',  # files/p16/p16631485/s50808053.txt\n",
    "        's51966317': 'indication',  # files/p10/p10817099/s51966317.txt\n",
    "        's50743547': 'examination',  # files/p11/p11388341/s50743547.txt\n",
    "        's56451190': 'note',  # files/p11/p11842879/s56451190.txt\n",
    "        's59067458': 'recommendations',  # files/p11/p11984647/s59067458.txt\n",
    "        's59215320': 'examination',  # files/p12/p12408912/s59215320.txt\n",
    "        's55124749': 'indication',  # files/p12/p12428492/s55124749.txt\n",
    "        's54365831': 'indication',  # files/p13/p13876470/s54365831.txt\n",
    "        's59087630': 'recommendations',  # files/p14/p14267880/s59087630.txt\n",
    "        's58157373': 'recommendations',  # files/p15/p15032392/s58157373.txt\n",
    "        's56482935': 'recommendations',  # files/p15/p15388421/s56482935.txt\n",
    "        's58375018': 'recommendations',  # files/p15/p15505556/s58375018.txt\n",
    "        's54654948': 'indication',  # files/p17/p17090359/s54654948.txt\n",
    "        's55157853': 'examination',  # files/p18/p18975498/s55157853.txt\n",
    "        's51491012': 'history',  # files/p19/p19314266/s51491012.txt\n",
    "\n",
    "    }\n",
    "\n",
    "    custom_indices = {\n",
    "        's50525523': [201, 349],  # files/p10/p10602608/s50525523.txt\n",
    "        's57564132': [233, 554],  # files/p10/p10637168/s57564132.txt\n",
    "        's59982525': [313, 717],  # files/p11/p11989982/s59982525.txt\n",
    "        's53488209': [149, 475],  # files/p12/p12458657/s53488209.txt\n",
    "        's54875119': [234, 988],  # files/p13/p13687044/s54875119.txt\n",
    "        's50196495': [59, 399],  # files/p13/p13894879/s50196495.txt\n",
    "        's56579911': [59, 218],  # files/p15/p15394326/s56579911.txt\n",
    "        's52648681': [292, 631],  # files/p15/p15666238/s52648681.txt\n",
    "        's59889364': [172, 453],  # files/p15/p15835529/s59889364.txt\n",
    "        's53514462': [73, 377],  # files/p16/p16297706/s53514462.txt\n",
    "        's59505494': [59, 450],  # files/p16/p16730991/s59505494.txt\n",
    "        's53182247': [59, 412],  # files/p16/p16770442/s53182247.txt\n",
    "        's51410602': [47, 320],  # files/p17/p17069955/s51410602.txt\n",
    "        's56412866': [522, 822],  # files/p17/p17612000/s56412866.txt\n",
    "        's54986978': [59, 306],  # files/p17/p17912487/s54986978.txt\n",
    "        's59003148': [262, 505],  # files/p17/p17916384/s59003148.txt\n",
    "        's57150433': [61, 394],  # files/p18/p18335791/s57150433.txt\n",
    "        's56760320': [219, 457],  # files/p18/p18418794/s56760320.txt\n",
    "        's59562049': [158, 348],  # files/p18/p18502016/s59562049.txt\n",
    "        's52674888': [145, 296],  # files/p19/p19381919/s52674888.txt\n",
    "        's55258338': [192, 568],  # files/p13/p13719117/s55258338.txt\n",
    "        's59330497': [140, 655],  # files/p15/p15479218/s59330497.txt\n",
    "        's52119491': [179, 454],  # files/p17/p17959278/s52119491.txt\n",
    "        # below have no findings at all in the entire report\n",
    "        's58235663': [0, 0],  # files/p11/p11573679/s58235663.txt\n",
    "        's50798377': [0, 0],  # files/p12/p12632853/s50798377.txt\n",
    "        's54168089': [0, 0],  # files/p14/p14463099/s54168089.txt\n",
    "        's53071062': [0, 0],  # files/p15/p15774521/s53071062.txt\n",
    "        's56724958': [0, 0],  # files/p16/p16175671/s56724958.txt\n",
    "        's54231141': [0, 0],  # files/p16/p16312859/s54231141.txt\n",
    "        's53607029': [0, 0],  # files/p17/p17603668/s53607029.txt\n",
    "        's52035334': [0, 0],  # files/p19/p19349312/s52035334.txt\n",
    "    }\n",
    "\n",
    "    return custom_section_names, custom_indices\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
