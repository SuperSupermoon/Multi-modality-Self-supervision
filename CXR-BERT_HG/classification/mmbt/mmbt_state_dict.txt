MMBT model : 
enc.txt_embeddings.position_ids          torch.Size([1, 512])
enc.txt_embeddings.word_embeddings.weight        torch.Size([30522, 512])
enc.txt_embeddings.position_embeddings.weight    torch.Size([512, 512])
enc.txt_embeddings.token_type_embeddings.weight          torch.Size([2, 512])
enc.txt_embeddings.LayerNorm.weight      torch.Size([512])
enc.txt_embeddings.LayerNorm.bias        torch.Size([512])
enc.img_embeddings.img_embeddings.weight         torch.Size([512, 2048])
enc.img_embeddings.img_embeddings.bias   torch.Size([512])
enc.img_embeddings.position_embeddings.weight    torch.Size([512, 512])
enc.img_embeddings.token_type_embeddings.weight          torch.Size([2, 512])
enc.img_embeddings.word_embeddings.weight        torch.Size([30522, 512])
enc.img_embeddings.LayerNorm.weight      torch.Size([512])
enc.img_embeddings.LayerNorm.bias        torch.Size([512])
enc.img_encoder.model.0.weight   torch.Size([64, 3, 7, 7])
enc.img_encoder.model.1.weight   torch.Size([64])
enc.img_encoder.model.1.bias     torch.Size([64])
enc.img_encoder.model.1.running_mean     torch.Size([64])
enc.img_encoder.model.1.running_var      torch.Size([64])
enc.img_encoder.model.1.num_batches_tracked      torch.Size([])
enc.img_encoder.model.4.0.conv1.weight   torch.Size([64, 64, 1, 1])
enc.img_encoder.model.4.0.bn1.weight     torch.Size([64])
enc.img_encoder.model.4.0.bn1.bias       torch.Size([64])
enc.img_encoder.model.4.0.bn1.running_mean       torch.Size([64])
enc.img_encoder.model.4.0.bn1.running_var        torch.Size([64])
enc.img_encoder.model.4.0.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.4.0.conv2.weight   torch.Size([64, 64, 3, 3])
enc.img_encoder.model.4.0.bn2.weight     torch.Size([64])
enc.img_encoder.model.4.0.bn2.bias       torch.Size([64])
enc.img_encoder.model.4.0.bn2.running_mean       torch.Size([64])
enc.img_encoder.model.4.0.bn2.running_var        torch.Size([64])
enc.img_encoder.model.4.0.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.4.0.conv3.weight   torch.Size([256, 64, 1, 1])
enc.img_encoder.model.4.0.bn3.weight     torch.Size([256])
enc.img_encoder.model.4.0.bn3.bias       torch.Size([256])
enc.img_encoder.model.4.0.bn3.running_mean       torch.Size([256])
enc.img_encoder.model.4.0.bn3.running_var        torch.Size([256])
enc.img_encoder.model.4.0.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.4.0.downsample.0.weight    torch.Size([256, 64, 1, 1])
enc.img_encoder.model.4.0.downsample.1.weight    torch.Size([256])
enc.img_encoder.model.4.0.downsample.1.bias      torch.Size([256])
enc.img_encoder.model.4.0.downsample.1.running_mean      torch.Size([256])
enc.img_encoder.model.4.0.downsample.1.running_var       torch.Size([256])
enc.img_encoder.model.4.0.downsample.1.num_batches_tracked       torch.Size([])
enc.img_encoder.model.4.1.conv1.weight   torch.Size([64, 256, 1, 1])
enc.img_encoder.model.4.1.bn1.weight     torch.Size([64])
enc.img_encoder.model.4.1.bn1.bias       torch.Size([64])
enc.img_encoder.model.4.1.bn1.running_mean       torch.Size([64])
enc.img_encoder.model.4.1.bn1.running_var        torch.Size([64])
enc.img_encoder.model.4.1.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.4.1.conv2.weight   torch.Size([64, 64, 3, 3])
enc.img_encoder.model.4.1.bn2.weight     torch.Size([64])
enc.img_encoder.model.4.1.bn2.bias       torch.Size([64])
enc.img_encoder.model.4.1.bn2.running_mean       torch.Size([64])
enc.img_encoder.model.4.1.bn2.running_var        torch.Size([64])
enc.img_encoder.model.4.1.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.4.1.conv3.weight   torch.Size([256, 64, 1, 1])
enc.img_encoder.model.4.1.bn3.weight     torch.Size([256])
enc.img_encoder.model.4.1.bn3.bias       torch.Size([256])
enc.img_encoder.model.4.1.bn3.running_mean       torch.Size([256])
enc.img_encoder.model.4.1.bn3.running_var        torch.Size([256])
enc.img_encoder.model.4.1.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.4.2.conv1.weight   torch.Size([64, 256, 1, 1])
enc.img_encoder.model.4.2.bn1.weight     torch.Size([64])
enc.img_encoder.model.4.2.bn1.bias       torch.Size([64])
enc.img_encoder.model.4.2.bn1.running_mean       torch.Size([64])
enc.img_encoder.model.4.2.bn1.running_var        torch.Size([64])
enc.img_encoder.model.4.2.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.4.2.conv2.weight   torch.Size([64, 64, 3, 3])
enc.img_encoder.model.4.2.bn2.weight     torch.Size([64])
enc.img_encoder.model.4.2.bn2.bias       torch.Size([64])
enc.img_encoder.model.4.2.bn2.running_mean       torch.Size([64])
enc.img_encoder.model.4.2.bn2.running_var        torch.Size([64])
enc.img_encoder.model.4.2.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.4.2.conv3.weight   torch.Size([256, 64, 1, 1])
enc.img_encoder.model.4.2.bn3.weight     torch.Size([256])
enc.img_encoder.model.4.2.bn3.bias       torch.Size([256])
enc.img_encoder.model.4.2.bn3.running_mean       torch.Size([256])
enc.img_encoder.model.4.2.bn3.running_var        torch.Size([256])
enc.img_encoder.model.4.2.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.5.0.conv1.weight   torch.Size([128, 256, 1, 1])
enc.img_encoder.model.5.0.bn1.weight     torch.Size([128])
enc.img_encoder.model.5.0.bn1.bias       torch.Size([128])
enc.img_encoder.model.5.0.bn1.running_mean       torch.Size([128])
enc.img_encoder.model.5.0.bn1.running_var        torch.Size([128])
enc.img_encoder.model.5.0.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.5.0.conv2.weight   torch.Size([128, 128, 3, 3])
enc.img_encoder.model.5.0.bn2.weight     torch.Size([128])
enc.img_encoder.model.5.0.bn2.bias       torch.Size([128])
enc.img_encoder.model.5.0.bn2.running_mean       torch.Size([128])
enc.img_encoder.model.5.0.bn2.running_var        torch.Size([128])
enc.img_encoder.model.5.0.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.5.0.conv3.weight   torch.Size([512, 128, 1, 1])
enc.img_encoder.model.5.0.bn3.weight     torch.Size([512])
enc.img_encoder.model.5.0.bn3.bias       torch.Size([512])
enc.img_encoder.model.5.0.bn3.running_mean       torch.Size([512])
enc.img_encoder.model.5.0.bn3.running_var        torch.Size([512])
enc.img_encoder.model.5.0.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.5.0.downsample.0.weight    torch.Size([512, 256, 1, 1])
enc.img_encoder.model.5.0.downsample.1.weight    torch.Size([512])
enc.img_encoder.model.5.0.downsample.1.bias      torch.Size([512])
enc.img_encoder.model.5.0.downsample.1.running_mean      torch.Size([512])
enc.img_encoder.model.5.0.downsample.1.running_var       torch.Size([512])
enc.img_encoder.model.5.0.downsample.1.num_batches_tracked       torch.Size([])
enc.img_encoder.model.5.1.conv1.weight   torch.Size([128, 512, 1, 1])
enc.img_encoder.model.5.1.bn1.weight     torch.Size([128])
enc.img_encoder.model.5.1.bn1.bias       torch.Size([128])
enc.img_encoder.model.5.1.bn1.running_mean       torch.Size([128])
enc.img_encoder.model.5.1.bn1.running_var        torch.Size([128])
enc.img_encoder.model.5.1.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.5.1.conv2.weight   torch.Size([128, 128, 3, 3])
enc.img_encoder.model.5.1.bn2.weight     torch.Size([128])
enc.img_encoder.model.5.1.bn2.bias       torch.Size([128])
enc.img_encoder.model.5.1.bn2.running_mean       torch.Size([128])
enc.img_encoder.model.5.1.bn2.running_var        torch.Size([128])
enc.img_encoder.model.5.1.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.5.1.conv3.weight   torch.Size([512, 128, 1, 1])
enc.img_encoder.model.5.1.bn3.weight     torch.Size([512])
enc.img_encoder.model.5.1.bn3.bias       torch.Size([512])
enc.img_encoder.model.5.1.bn3.running_mean       torch.Size([512])
enc.img_encoder.model.5.1.bn3.running_var        torch.Size([512])
enc.img_encoder.model.5.1.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.5.2.conv1.weight   torch.Size([128, 512, 1, 1])
enc.img_encoder.model.5.2.bn1.weight     torch.Size([128])
enc.img_encoder.model.5.2.bn1.bias       torch.Size([128])
enc.img_encoder.model.5.2.bn1.running_mean       torch.Size([128])
enc.img_encoder.model.5.2.bn1.running_var        torch.Size([128])
enc.img_encoder.model.5.2.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.5.2.conv2.weight   torch.Size([128, 128, 3, 3])
enc.img_encoder.model.5.2.bn2.weight     torch.Size([128])
enc.img_encoder.model.5.2.bn2.bias       torch.Size([128])
enc.img_encoder.model.5.2.bn2.running_mean       torch.Size([128])
enc.img_encoder.model.5.2.bn2.running_var        torch.Size([128])
enc.img_encoder.model.5.2.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.5.2.conv3.weight   torch.Size([512, 128, 1, 1])
enc.img_encoder.model.5.2.bn3.weight     torch.Size([512])
enc.img_encoder.model.5.2.bn3.bias       torch.Size([512])
enc.img_encoder.model.5.2.bn3.running_mean       torch.Size([512])
enc.img_encoder.model.5.2.bn3.running_var        torch.Size([512])
enc.img_encoder.model.5.2.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.5.3.conv1.weight   torch.Size([128, 512, 1, 1])
enc.img_encoder.model.5.3.bn1.weight     torch.Size([128])
enc.img_encoder.model.5.3.bn1.bias       torch.Size([128])
enc.img_encoder.model.5.3.bn1.running_mean       torch.Size([128])
enc.img_encoder.model.5.3.bn1.running_var        torch.Size([128])
enc.img_encoder.model.5.3.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.5.3.conv2.weight   torch.Size([128, 128, 3, 3])
enc.img_encoder.model.5.3.bn2.weight     torch.Size([128])
enc.img_encoder.model.5.3.bn2.bias       torch.Size([128])
enc.img_encoder.model.5.3.bn2.running_mean       torch.Size([128])
enc.img_encoder.model.5.3.bn2.running_var        torch.Size([128])
enc.img_encoder.model.5.3.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.5.3.conv3.weight   torch.Size([512, 128, 1, 1])
enc.img_encoder.model.5.3.bn3.weight     torch.Size([512])
enc.img_encoder.model.5.3.bn3.bias       torch.Size([512])
enc.img_encoder.model.5.3.bn3.running_mean       torch.Size([512])
enc.img_encoder.model.5.3.bn3.running_var        torch.Size([512])
enc.img_encoder.model.5.3.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.0.conv1.weight   torch.Size([256, 512, 1, 1])
enc.img_encoder.model.6.0.bn1.weight     torch.Size([256])
enc.img_encoder.model.6.0.bn1.bias       torch.Size([256])
enc.img_encoder.model.6.0.bn1.running_mean       torch.Size([256])
enc.img_encoder.model.6.0.bn1.running_var        torch.Size([256])
enc.img_encoder.model.6.0.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.0.conv2.weight   torch.Size([256, 256, 3, 3])
enc.img_encoder.model.6.0.bn2.weight     torch.Size([256])
enc.img_encoder.model.6.0.bn2.bias       torch.Size([256])
enc.img_encoder.model.6.0.bn2.running_mean       torch.Size([256])
enc.img_encoder.model.6.0.bn2.running_var        torch.Size([256])
enc.img_encoder.model.6.0.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.0.conv3.weight   torch.Size([1024, 256, 1, 1])
enc.img_encoder.model.6.0.bn3.weight     torch.Size([1024])
enc.img_encoder.model.6.0.bn3.bias       torch.Size([1024])
enc.img_encoder.model.6.0.bn3.running_mean       torch.Size([1024])
enc.img_encoder.model.6.0.bn3.running_var        torch.Size([1024])
enc.img_encoder.model.6.0.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.0.downsample.0.weight    torch.Size([1024, 512, 1, 1])
enc.img_encoder.model.6.0.downsample.1.weight    torch.Size([1024])
enc.img_encoder.model.6.0.downsample.1.bias      torch.Size([1024])
enc.img_encoder.model.6.0.downsample.1.running_mean      torch.Size([1024])
enc.img_encoder.model.6.0.downsample.1.running_var       torch.Size([1024])
enc.img_encoder.model.6.0.downsample.1.num_batches_tracked       torch.Size([])
enc.img_encoder.model.6.1.conv1.weight   torch.Size([256, 1024, 1, 1])
enc.img_encoder.model.6.1.bn1.weight     torch.Size([256])
enc.img_encoder.model.6.1.bn1.bias       torch.Size([256])
enc.img_encoder.model.6.1.bn1.running_mean       torch.Size([256])
enc.img_encoder.model.6.1.bn1.running_var        torch.Size([256])
enc.img_encoder.model.6.1.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.1.conv2.weight   torch.Size([256, 256, 3, 3])
enc.img_encoder.model.6.1.bn2.weight     torch.Size([256])
enc.img_encoder.model.6.1.bn2.bias       torch.Size([256])
enc.img_encoder.model.6.1.bn2.running_mean       torch.Size([256])
enc.img_encoder.model.6.1.bn2.running_var        torch.Size([256])
enc.img_encoder.model.6.1.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.1.conv3.weight   torch.Size([1024, 256, 1, 1])
enc.img_encoder.model.6.1.bn3.weight     torch.Size([1024])
enc.img_encoder.model.6.1.bn3.bias       torch.Size([1024])
enc.img_encoder.model.6.1.bn3.running_mean       torch.Size([1024])
enc.img_encoder.model.6.1.bn3.running_var        torch.Size([1024])
enc.img_encoder.model.6.1.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.2.conv1.weight   torch.Size([256, 1024, 1, 1])
enc.img_encoder.model.6.2.bn1.weight     torch.Size([256])
enc.img_encoder.model.6.2.bn1.bias       torch.Size([256])
enc.img_encoder.model.6.2.bn1.running_mean       torch.Size([256])
enc.img_encoder.model.6.2.bn1.running_var        torch.Size([256])
enc.img_encoder.model.6.2.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.2.conv2.weight   torch.Size([256, 256, 3, 3])
enc.img_encoder.model.6.2.bn2.weight     torch.Size([256])
enc.img_encoder.model.6.2.bn2.bias       torch.Size([256])
enc.img_encoder.model.6.2.bn2.running_mean       torch.Size([256])
enc.img_encoder.model.6.2.bn2.running_var        torch.Size([256])
enc.img_encoder.model.6.2.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.2.conv3.weight   torch.Size([1024, 256, 1, 1])
enc.img_encoder.model.6.2.bn3.weight     torch.Size([1024])
enc.img_encoder.model.6.2.bn3.bias       torch.Size([1024])
enc.img_encoder.model.6.2.bn3.running_mean       torch.Size([1024])
enc.img_encoder.model.6.2.bn3.running_var        torch.Size([1024])
enc.img_encoder.model.6.2.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.3.conv1.weight   torch.Size([256, 1024, 1, 1])
enc.img_encoder.model.6.3.bn1.weight     torch.Size([256])
enc.img_encoder.model.6.3.bn1.bias       torch.Size([256])
enc.img_encoder.model.6.3.bn1.running_mean       torch.Size([256])
enc.img_encoder.model.6.3.bn1.running_var        torch.Size([256])
enc.img_encoder.model.6.3.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.3.conv2.weight   torch.Size([256, 256, 3, 3])
enc.img_encoder.model.6.3.bn2.weight     torch.Size([256])
enc.img_encoder.model.6.3.bn2.bias       torch.Size([256])
enc.img_encoder.model.6.3.bn2.running_mean       torch.Size([256])
enc.img_encoder.model.6.3.bn2.running_var        torch.Size([256])
enc.img_encoder.model.6.3.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.3.conv3.weight   torch.Size([1024, 256, 1, 1])
enc.img_encoder.model.6.3.bn3.weight     torch.Size([1024])
enc.img_encoder.model.6.3.bn3.bias       torch.Size([1024])
enc.img_encoder.model.6.3.bn3.running_mean       torch.Size([1024])
enc.img_encoder.model.6.3.bn3.running_var        torch.Size([1024])
enc.img_encoder.model.6.3.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.4.conv1.weight   torch.Size([256, 1024, 1, 1])
enc.img_encoder.model.6.4.bn1.weight     torch.Size([256])
enc.img_encoder.model.6.4.bn1.bias       torch.Size([256])
enc.img_encoder.model.6.4.bn1.running_mean       torch.Size([256])
enc.img_encoder.model.6.4.bn1.running_var        torch.Size([256])
enc.img_encoder.model.6.4.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.4.conv2.weight   torch.Size([256, 256, 3, 3])
enc.img_encoder.model.6.4.bn2.weight     torch.Size([256])
enc.img_encoder.model.6.4.bn2.bias       torch.Size([256])
enc.img_encoder.model.6.4.bn2.running_mean       torch.Size([256])
enc.img_encoder.model.6.4.bn2.running_var        torch.Size([256])
enc.img_encoder.model.6.4.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.4.conv3.weight   torch.Size([1024, 256, 1, 1])
enc.img_encoder.model.6.4.bn3.weight     torch.Size([1024])
enc.img_encoder.model.6.4.bn3.bias       torch.Size([1024])
enc.img_encoder.model.6.4.bn3.running_mean       torch.Size([1024])
enc.img_encoder.model.6.4.bn3.running_var        torch.Size([1024])
enc.img_encoder.model.6.4.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.5.conv1.weight   torch.Size([256, 1024, 1, 1])
enc.img_encoder.model.6.5.bn1.weight     torch.Size([256])
enc.img_encoder.model.6.5.bn1.bias       torch.Size([256])
enc.img_encoder.model.6.5.bn1.running_mean       torch.Size([256])
enc.img_encoder.model.6.5.bn1.running_var        torch.Size([256])
enc.img_encoder.model.6.5.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.5.conv2.weight   torch.Size([256, 256, 3, 3])
enc.img_encoder.model.6.5.bn2.weight     torch.Size([256])
enc.img_encoder.model.6.5.bn2.bias       torch.Size([256])
enc.img_encoder.model.6.5.bn2.running_mean       torch.Size([256])
enc.img_encoder.model.6.5.bn2.running_var        torch.Size([256])
enc.img_encoder.model.6.5.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.6.5.conv3.weight   torch.Size([1024, 256, 1, 1])
enc.img_encoder.model.6.5.bn3.weight     torch.Size([1024])
enc.img_encoder.model.6.5.bn3.bias       torch.Size([1024])
enc.img_encoder.model.6.5.bn3.running_mean       torch.Size([1024])
enc.img_encoder.model.6.5.bn3.running_var        torch.Size([1024])
enc.img_encoder.model.6.5.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.7.0.conv1.weight   torch.Size([512, 1024, 1, 1])
enc.img_encoder.model.7.0.bn1.weight     torch.Size([512])
enc.img_encoder.model.7.0.bn1.bias       torch.Size([512])
enc.img_encoder.model.7.0.bn1.running_mean       torch.Size([512])
enc.img_encoder.model.7.0.bn1.running_var        torch.Size([512])
enc.img_encoder.model.7.0.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.7.0.conv2.weight   torch.Size([512, 512, 3, 3])
enc.img_encoder.model.7.0.bn2.weight     torch.Size([512])
enc.img_encoder.model.7.0.bn2.bias       torch.Size([512])
enc.img_encoder.model.7.0.bn2.running_mean       torch.Size([512])
enc.img_encoder.model.7.0.bn2.running_var        torch.Size([512])
enc.img_encoder.model.7.0.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.7.0.conv3.weight   torch.Size([2048, 512, 1, 1])
enc.img_encoder.model.7.0.bn3.weight     torch.Size([2048])
enc.img_encoder.model.7.0.bn3.bias       torch.Size([2048])
enc.img_encoder.model.7.0.bn3.running_mean       torch.Size([2048])
enc.img_encoder.model.7.0.bn3.running_var        torch.Size([2048])
enc.img_encoder.model.7.0.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.7.0.downsample.0.weight    torch.Size([2048, 1024, 1, 1])
enc.img_encoder.model.7.0.downsample.1.weight    torch.Size([2048])
enc.img_encoder.model.7.0.downsample.1.bias      torch.Size([2048])
enc.img_encoder.model.7.0.downsample.1.running_mean      torch.Size([2048])
enc.img_encoder.model.7.0.downsample.1.running_var       torch.Size([2048])
enc.img_encoder.model.7.0.downsample.1.num_batches_tracked       torch.Size([])
enc.img_encoder.model.7.1.conv1.weight   torch.Size([512, 2048, 1, 1])
enc.img_encoder.model.7.1.bn1.weight     torch.Size([512])
enc.img_encoder.model.7.1.bn1.bias       torch.Size([512])
enc.img_encoder.model.7.1.bn1.running_mean       torch.Size([512])
enc.img_encoder.model.7.1.bn1.running_var        torch.Size([512])
enc.img_encoder.model.7.1.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.7.1.conv2.weight   torch.Size([512, 512, 3, 3])
enc.img_encoder.model.7.1.bn2.weight     torch.Size([512])
enc.img_encoder.model.7.1.bn2.bias       torch.Size([512])
enc.img_encoder.model.7.1.bn2.running_mean       torch.Size([512])
enc.img_encoder.model.7.1.bn2.running_var        torch.Size([512])
enc.img_encoder.model.7.1.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.7.1.conv3.weight   torch.Size([2048, 512, 1, 1])
enc.img_encoder.model.7.1.bn3.weight     torch.Size([2048])
enc.img_encoder.model.7.1.bn3.bias       torch.Size([2048])
enc.img_encoder.model.7.1.bn3.running_mean       torch.Size([2048])
enc.img_encoder.model.7.1.bn3.running_var        torch.Size([2048])
enc.img_encoder.model.7.1.bn3.num_batches_tracked        torch.Size([])
enc.img_encoder.model.7.2.conv1.weight   torch.Size([512, 2048, 1, 1])
enc.img_encoder.model.7.2.bn1.weight     torch.Size([512])
enc.img_encoder.model.7.2.bn1.bias       torch.Size([512])
enc.img_encoder.model.7.2.bn1.running_mean       torch.Size([512])
enc.img_encoder.model.7.2.bn1.running_var        torch.Size([512])
enc.img_encoder.model.7.2.bn1.num_batches_tracked        torch.Size([])
enc.img_encoder.model.7.2.conv2.weight   torch.Size([512, 512, 3, 3])
enc.img_encoder.model.7.2.bn2.weight     torch.Size([512])
enc.img_encoder.model.7.2.bn2.bias       torch.Size([512])
enc.img_encoder.model.7.2.bn2.running_mean       torch.Size([512])
enc.img_encoder.model.7.2.bn2.running_var        torch.Size([512])
enc.img_encoder.model.7.2.bn2.num_batches_tracked        torch.Size([])
enc.img_encoder.model.7.2.conv3.weight   torch.Size([2048, 512, 1, 1])
enc.img_encoder.model.7.2.bn3.weight     torch.Size([2048])
enc.img_encoder.model.7.2.bn3.bias       torch.Size([2048])
enc.img_encoder.model.7.2.bn3.running_mean       torch.Size([2048])
enc.img_encoder.model.7.2.bn3.running_var        torch.Size([2048])
enc.img_encoder.model.7.2.bn3.num_batches_tracked        torch.Size([])
enc.encoder.layer.0.attention.self.query.weight          torch.Size([512, 512])
enc.encoder.layer.0.attention.self.query.bias    torch.Size([512])
enc.encoder.layer.0.attention.self.key.weight    torch.Size([512, 512])
enc.encoder.layer.0.attention.self.key.bias      torch.Size([512])
enc.encoder.layer.0.attention.self.value.weight          torch.Size([512, 512])
enc.encoder.layer.0.attention.self.value.bias    torch.Size([512])
enc.encoder.layer.0.attention.output.dense.weight        torch.Size([512, 512])
enc.encoder.layer.0.attention.output.dense.bias          torch.Size([512])
enc.encoder.layer.0.attention.output.LayerNorm.weight    torch.Size([512])
enc.encoder.layer.0.attention.output.LayerNorm.bias      torch.Size([512])
enc.encoder.layer.0.intermediate.dense.weight    torch.Size([2048, 512])
enc.encoder.layer.0.intermediate.dense.bias      torch.Size([2048])
enc.encoder.layer.0.output.dense.weight          torch.Size([512, 2048])
enc.encoder.layer.0.output.dense.bias    torch.Size([512])
enc.encoder.layer.0.output.LayerNorm.weight      torch.Size([512])
enc.encoder.layer.0.output.LayerNorm.bias        torch.Size([512])
enc.encoder.layer.1.attention.self.query.weight          torch.Size([512, 512])
enc.encoder.layer.1.attention.self.query.bias    torch.Size([512])
enc.encoder.layer.1.attention.self.key.weight    torch.Size([512, 512])
enc.encoder.layer.1.attention.self.key.bias      torch.Size([512])
enc.encoder.layer.1.attention.self.value.weight          torch.Size([512, 512])
enc.encoder.layer.1.attention.self.value.bias    torch.Size([512])
enc.encoder.layer.1.attention.output.dense.weight        torch.Size([512, 512])
enc.encoder.layer.1.attention.output.dense.bias          torch.Size([512])
enc.encoder.layer.1.attention.output.LayerNorm.weight    torch.Size([512])
enc.encoder.layer.1.attention.output.LayerNorm.bias      torch.Size([512])
enc.encoder.layer.1.intermediate.dense.weight    torch.Size([2048, 512])
enc.encoder.layer.1.intermediate.dense.bias      torch.Size([2048])
enc.encoder.layer.1.output.dense.weight          torch.Size([512, 2048])
enc.encoder.layer.1.output.dense.bias    torch.Size([512])
enc.encoder.layer.1.output.LayerNorm.weight      torch.Size([512])
enc.encoder.layer.1.output.LayerNorm.bias        torch.Size([512])
enc.encoder.layer.2.attention.self.query.weight          torch.Size([512, 512])
enc.encoder.layer.2.attention.self.query.bias    torch.Size([512])
enc.encoder.layer.2.attention.self.key.weight    torch.Size([512, 512])
enc.encoder.layer.2.attention.self.key.bias      torch.Size([512])
enc.encoder.layer.2.attention.self.value.weight          torch.Size([512, 512])
enc.encoder.layer.2.attention.self.value.bias    torch.Size([512])
enc.encoder.layer.2.attention.output.dense.weight        torch.Size([512, 512])
enc.encoder.layer.2.attention.output.dense.bias          torch.Size([512])
enc.encoder.layer.2.attention.output.LayerNorm.weight    torch.Size([512])
enc.encoder.layer.2.attention.output.LayerNorm.bias      torch.Size([512])
enc.encoder.layer.2.intermediate.dense.weight    torch.Size([2048, 512])
enc.encoder.layer.2.intermediate.dense.bias      torch.Size([2048])
enc.encoder.layer.2.output.dense.weight          torch.Size([512, 2048])
enc.encoder.layer.2.output.dense.bias    torch.Size([512])
enc.encoder.layer.2.output.LayerNorm.weight      torch.Size([512])
enc.encoder.layer.2.output.LayerNorm.bias        torch.Size([512])
enc.encoder.layer.3.attention.self.query.weight          torch.Size([512, 512])
enc.encoder.layer.3.attention.self.query.bias    torch.Size([512])
enc.encoder.layer.3.attention.self.key.weight    torch.Size([512, 512])
enc.encoder.layer.3.attention.self.key.bias      torch.Size([512])
enc.encoder.layer.3.attention.self.value.weight          torch.Size([512, 512])
enc.encoder.layer.3.attention.self.value.bias    torch.Size([512])
enc.encoder.layer.3.attention.output.dense.weight        torch.Size([512, 512])
enc.encoder.layer.3.attention.output.dense.bias          torch.Size([512])
enc.encoder.layer.3.attention.output.LayerNorm.weight    torch.Size([512])
enc.encoder.layer.3.attention.output.LayerNorm.bias      torch.Size([512])
enc.encoder.layer.3.intermediate.dense.weight    torch.Size([2048, 512])
enc.encoder.layer.3.intermediate.dense.bias      torch.Size([2048])
enc.encoder.layer.3.output.dense.weight          torch.Size([512, 2048])
enc.encoder.layer.3.output.dense.bias    torch.Size([512])
enc.encoder.layer.3.output.LayerNorm.weight      torch.Size([512])
enc.encoder.layer.3.output.LayerNorm.bias        torch.Size([512])
enc.pooler.dense.weight          torch.Size([512, 512])
enc.pooler.dense.bias    torch.Size([512])
enc.clf.weight   torch.Size([15, 512])
enc.clf.bias     torch.Size([15])
clf.weight       torch.Size([15, 512])
clf.bias         torch.Size([15])