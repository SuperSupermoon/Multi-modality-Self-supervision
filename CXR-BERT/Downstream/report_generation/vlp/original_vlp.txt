original vlp model's statedict : 
txt_embeddings.word_embeddings.weight    torch.Size([30522, 512])
txt_embeddings.position_embeddings.weight        torch.Size([512, 512])
txt_embeddings.token_type_embeddings.weight      torch.Size([6, 512])
txt_embeddings.LayerNorm.weight          torch.Size([512])
txt_embeddings.LayerNorm.bias    torch.Size([512])
img_embeddings.img_embeddings.weight     torch.Size([512, 2048])
img_embeddings.img_embeddings.bias       torch.Size([512])
img_embeddings.position_embeddings.weight        torch.Size([512, 512])
img_embeddings.token_type_embeddings.weight      torch.Size([6, 512])
img_embeddings.word_embeddings.weight    torch.Size([30522, 512])
img_embeddings.LayerNorm.weight          torch.Size([512])
img_embeddings.LayerNorm.bias    torch.Size([512])
img_encoder.model.0.weight       torch.Size([64, 3, 7, 7])
img_encoder.model.1.weight       torch.Size([64])
img_encoder.model.1.bias         torch.Size([64])
img_encoder.model.1.running_mean         torch.Size([64])
img_encoder.model.1.running_var          torch.Size([64])
img_encoder.model.1.num_batches_tracked          torch.Size([])
img_encoder.model.4.0.conv1.weight       torch.Size([64, 64, 1, 1])
img_encoder.model.4.0.bn1.weight         torch.Size([64])
img_encoder.model.4.0.bn1.bias   torch.Size([64])
img_encoder.model.4.0.bn1.running_mean   torch.Size([64])
img_encoder.model.4.0.bn1.running_var    torch.Size([64])
img_encoder.model.4.0.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.4.0.conv2.weight       torch.Size([64, 64, 3, 3])
img_encoder.model.4.0.bn2.weight         torch.Size([64])
img_encoder.model.4.0.bn2.bias   torch.Size([64])
img_encoder.model.4.0.bn2.running_mean   torch.Size([64])
img_encoder.model.4.0.bn2.running_var    torch.Size([64])
img_encoder.model.4.0.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.4.0.conv3.weight       torch.Size([256, 64, 1, 1])
img_encoder.model.4.0.bn3.weight         torch.Size([256])
img_encoder.model.4.0.bn3.bias   torch.Size([256])
img_encoder.model.4.0.bn3.running_mean   torch.Size([256])
img_encoder.model.4.0.bn3.running_var    torch.Size([256])
img_encoder.model.4.0.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.4.0.downsample.0.weight        torch.Size([256, 64, 1, 1])
img_encoder.model.4.0.downsample.1.weight        torch.Size([256])
img_encoder.model.4.0.downsample.1.bias          torch.Size([256])
img_encoder.model.4.0.downsample.1.running_mean          torch.Size([256])
img_encoder.model.4.0.downsample.1.running_var   torch.Size([256])
img_encoder.model.4.0.downsample.1.num_batches_tracked   torch.Size([])
img_encoder.model.4.1.conv1.weight       torch.Size([64, 256, 1, 1])
img_encoder.model.4.1.bn1.weight         torch.Size([64])
img_encoder.model.4.1.bn1.bias   torch.Size([64])
img_encoder.model.4.1.bn1.running_mean   torch.Size([64])
img_encoder.model.4.1.bn1.running_var    torch.Size([64])
img_encoder.model.4.1.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.4.1.conv2.weight       torch.Size([64, 64, 3, 3])
img_encoder.model.4.1.bn2.weight         torch.Size([64])
img_encoder.model.4.1.bn2.bias   torch.Size([64])
img_encoder.model.4.1.bn2.running_mean   torch.Size([64])
img_encoder.model.4.1.bn2.running_var    torch.Size([64])
img_encoder.model.4.1.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.4.1.conv3.weight       torch.Size([256, 64, 1, 1])
img_encoder.model.4.1.bn3.weight         torch.Size([256])
img_encoder.model.4.1.bn3.bias   torch.Size([256])
img_encoder.model.4.1.bn3.running_mean   torch.Size([256])
img_encoder.model.4.1.bn3.running_var    torch.Size([256])
img_encoder.model.4.1.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.4.2.conv1.weight       torch.Size([64, 256, 1, 1])
img_encoder.model.4.2.bn1.weight         torch.Size([64])
img_encoder.model.4.2.bn1.bias   torch.Size([64])
img_encoder.model.4.2.bn1.running_mean   torch.Size([64])
img_encoder.model.4.2.bn1.running_var    torch.Size([64])
img_encoder.model.4.2.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.4.2.conv2.weight       torch.Size([64, 64, 3, 3])
img_encoder.model.4.2.bn2.weight         torch.Size([64])
img_encoder.model.4.2.bn2.bias   torch.Size([64])
img_encoder.model.4.2.bn2.running_mean   torch.Size([64])
img_encoder.model.4.2.bn2.running_var    torch.Size([64])
img_encoder.model.4.2.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.4.2.conv3.weight       torch.Size([256, 64, 1, 1])
img_encoder.model.4.2.bn3.weight         torch.Size([256])
img_encoder.model.4.2.bn3.bias   torch.Size([256])
img_encoder.model.4.2.bn3.running_mean   torch.Size([256])
img_encoder.model.4.2.bn3.running_var    torch.Size([256])
img_encoder.model.4.2.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.5.0.conv1.weight       torch.Size([128, 256, 1, 1])
img_encoder.model.5.0.bn1.weight         torch.Size([128])
img_encoder.model.5.0.bn1.bias   torch.Size([128])
img_encoder.model.5.0.bn1.running_mean   torch.Size([128])
img_encoder.model.5.0.bn1.running_var    torch.Size([128])
img_encoder.model.5.0.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.5.0.conv2.weight       torch.Size([128, 128, 3, 3])
img_encoder.model.5.0.bn2.weight         torch.Size([128])
img_encoder.model.5.0.bn2.bias   torch.Size([128])
img_encoder.model.5.0.bn2.running_mean   torch.Size([128])
img_encoder.model.5.0.bn2.running_var    torch.Size([128])
img_encoder.model.5.0.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.5.0.conv3.weight       torch.Size([512, 128, 1, 1])
img_encoder.model.5.0.bn3.weight         torch.Size([512])
img_encoder.model.5.0.bn3.bias   torch.Size([512])
img_encoder.model.5.0.bn3.running_mean   torch.Size([512])
img_encoder.model.5.0.bn3.running_var    torch.Size([512])
img_encoder.model.5.0.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.5.0.downsample.0.weight        torch.Size([512, 256, 1, 1])
img_encoder.model.5.0.downsample.1.weight        torch.Size([512])
img_encoder.model.5.0.downsample.1.bias          torch.Size([512])
img_encoder.model.5.0.downsample.1.running_mean          torch.Size([512])
img_encoder.model.5.0.downsample.1.running_var   torch.Size([512])
img_encoder.model.5.0.downsample.1.num_batches_tracked   torch.Size([])
img_encoder.model.5.1.conv1.weight       torch.Size([128, 512, 1, 1])
img_encoder.model.5.1.bn1.weight         torch.Size([128])
img_encoder.model.5.1.bn1.bias   torch.Size([128])
img_encoder.model.5.1.bn1.running_mean   torch.Size([128])
img_encoder.model.5.1.bn1.running_var    torch.Size([128])
img_encoder.model.5.1.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.5.1.conv2.weight       torch.Size([128, 128, 3, 3])
img_encoder.model.5.1.bn2.weight         torch.Size([128])
img_encoder.model.5.1.bn2.bias   torch.Size([128])
img_encoder.model.5.1.bn2.running_mean   torch.Size([128])
img_encoder.model.5.1.bn2.running_var    torch.Size([128])
img_encoder.model.5.1.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.5.1.conv3.weight       torch.Size([512, 128, 1, 1])
img_encoder.model.5.1.bn3.weight         torch.Size([512])
img_encoder.model.5.1.bn3.bias   torch.Size([512])
img_encoder.model.5.1.bn3.running_mean   torch.Size([512])
img_encoder.model.5.1.bn3.running_var    torch.Size([512])
img_encoder.model.5.1.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.5.2.conv1.weight       torch.Size([128, 512, 1, 1])
img_encoder.model.5.2.bn1.weight         torch.Size([128])
img_encoder.model.5.2.bn1.bias   torch.Size([128])
img_encoder.model.5.2.bn1.running_mean   torch.Size([128])
img_encoder.model.5.2.bn1.running_var    torch.Size([128])
img_encoder.model.5.2.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.5.2.conv2.weight       torch.Size([128, 128, 3, 3])
img_encoder.model.5.2.bn2.weight         torch.Size([128])
img_encoder.model.5.2.bn2.bias   torch.Size([128])
img_encoder.model.5.2.bn2.running_mean   torch.Size([128])
img_encoder.model.5.2.bn2.running_var    torch.Size([128])
img_encoder.model.5.2.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.5.2.conv3.weight       torch.Size([512, 128, 1, 1])
img_encoder.model.5.2.bn3.weight         torch.Size([512])
img_encoder.model.5.2.bn3.bias   torch.Size([512])
img_encoder.model.5.2.bn3.running_mean   torch.Size([512])
img_encoder.model.5.2.bn3.running_var    torch.Size([512])
img_encoder.model.5.2.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.5.3.conv1.weight       torch.Size([128, 512, 1, 1])
img_encoder.model.5.3.bn1.weight         torch.Size([128])
img_encoder.model.5.3.bn1.bias   torch.Size([128])
img_encoder.model.5.3.bn1.running_mean   torch.Size([128])
img_encoder.model.5.3.bn1.running_var    torch.Size([128])
img_encoder.model.5.3.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.5.3.conv2.weight       torch.Size([128, 128, 3, 3])
img_encoder.model.5.3.bn2.weight         torch.Size([128])
img_encoder.model.5.3.bn2.bias   torch.Size([128])
img_encoder.model.5.3.bn2.running_mean   torch.Size([128])
img_encoder.model.5.3.bn2.running_var    torch.Size([128])
img_encoder.model.5.3.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.5.3.conv3.weight       torch.Size([512, 128, 1, 1])
img_encoder.model.5.3.bn3.weight         torch.Size([512])
img_encoder.model.5.3.bn3.bias   torch.Size([512])
img_encoder.model.5.3.bn3.running_mean   torch.Size([512])
img_encoder.model.5.3.bn3.running_var    torch.Size([512])
img_encoder.model.5.3.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.6.0.conv1.weight       torch.Size([256, 512, 1, 1])
img_encoder.model.6.0.bn1.weight         torch.Size([256])
img_encoder.model.6.0.bn1.bias   torch.Size([256])
img_encoder.model.6.0.bn1.running_mean   torch.Size([256])
img_encoder.model.6.0.bn1.running_var    torch.Size([256])
img_encoder.model.6.0.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.6.0.conv2.weight       torch.Size([256, 256, 3, 3])
img_encoder.model.6.0.bn2.weight         torch.Size([256])
img_encoder.model.6.0.bn2.bias   torch.Size([256])
img_encoder.model.6.0.bn2.running_mean   torch.Size([256])
img_encoder.model.6.0.bn2.running_var    torch.Size([256])
img_encoder.model.6.0.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.6.0.conv3.weight       torch.Size([1024, 256, 1, 1])
img_encoder.model.6.0.bn3.weight         torch.Size([1024])
img_encoder.model.6.0.bn3.bias   torch.Size([1024])
img_encoder.model.6.0.bn3.running_mean   torch.Size([1024])
img_encoder.model.6.0.bn3.running_var    torch.Size([1024])
img_encoder.model.6.0.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.6.0.downsample.0.weight        torch.Size([1024, 512, 1, 1])
img_encoder.model.6.0.downsample.1.weight        torch.Size([1024])
img_encoder.model.6.0.downsample.1.bias          torch.Size([1024])
img_encoder.model.6.0.downsample.1.running_mean          torch.Size([1024])
img_encoder.model.6.0.downsample.1.running_var   torch.Size([1024])
img_encoder.model.6.0.downsample.1.num_batches_tracked   torch.Size([])
img_encoder.model.6.1.conv1.weight       torch.Size([256, 1024, 1, 1])
img_encoder.model.6.1.bn1.weight         torch.Size([256])
img_encoder.model.6.1.bn1.bias   torch.Size([256])
img_encoder.model.6.1.bn1.running_mean   torch.Size([256])
img_encoder.model.6.1.bn1.running_var    torch.Size([256])
img_encoder.model.6.1.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.6.1.conv2.weight       torch.Size([256, 256, 3, 3])
img_encoder.model.6.1.bn2.weight         torch.Size([256])
img_encoder.model.6.1.bn2.bias   torch.Size([256])
img_encoder.model.6.1.bn2.running_mean   torch.Size([256])
img_encoder.model.6.1.bn2.running_var    torch.Size([256])
img_encoder.model.6.1.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.6.1.conv3.weight       torch.Size([1024, 256, 1, 1])
img_encoder.model.6.1.bn3.weight         torch.Size([1024])
img_encoder.model.6.1.bn3.bias   torch.Size([1024])
img_encoder.model.6.1.bn3.running_mean   torch.Size([1024])
img_encoder.model.6.1.bn3.running_var    torch.Size([1024])
img_encoder.model.6.1.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.6.2.conv1.weight       torch.Size([256, 1024, 1, 1])
img_encoder.model.6.2.bn1.weight         torch.Size([256])
img_encoder.model.6.2.bn1.bias   torch.Size([256])
img_encoder.model.6.2.bn1.running_mean   torch.Size([256])
img_encoder.model.6.2.bn1.running_var    torch.Size([256])
img_encoder.model.6.2.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.6.2.conv2.weight       torch.Size([256, 256, 3, 3])
img_encoder.model.6.2.bn2.weight         torch.Size([256])
img_encoder.model.6.2.bn2.bias   torch.Size([256])
img_encoder.model.6.2.bn2.running_mean   torch.Size([256])
img_encoder.model.6.2.bn2.running_var    torch.Size([256])
img_encoder.model.6.2.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.6.2.conv3.weight       torch.Size([1024, 256, 1, 1])
img_encoder.model.6.2.bn3.weight         torch.Size([1024])
img_encoder.model.6.2.bn3.bias   torch.Size([1024])
img_encoder.model.6.2.bn3.running_mean   torch.Size([1024])
img_encoder.model.6.2.bn3.running_var    torch.Size([1024])
img_encoder.model.6.2.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.6.3.conv1.weight       torch.Size([256, 1024, 1, 1])
img_encoder.model.6.3.bn1.weight         torch.Size([256])
img_encoder.model.6.3.bn1.bias   torch.Size([256])
img_encoder.model.6.3.bn1.running_mean   torch.Size([256])
img_encoder.model.6.3.bn1.running_var    torch.Size([256])
img_encoder.model.6.3.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.6.3.conv2.weight       torch.Size([256, 256, 3, 3])
img_encoder.model.6.3.bn2.weight         torch.Size([256])
img_encoder.model.6.3.bn2.bias   torch.Size([256])
img_encoder.model.6.3.bn2.running_mean   torch.Size([256])
img_encoder.model.6.3.bn2.running_var    torch.Size([256])
img_encoder.model.6.3.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.6.3.conv3.weight       torch.Size([1024, 256, 1, 1])
img_encoder.model.6.3.bn3.weight         torch.Size([1024])
img_encoder.model.6.3.bn3.bias   torch.Size([1024])
img_encoder.model.6.3.bn3.running_mean   torch.Size([1024])
img_encoder.model.6.3.bn3.running_var    torch.Size([1024])
img_encoder.model.6.3.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.6.4.conv1.weight       torch.Size([256, 1024, 1, 1])
img_encoder.model.6.4.bn1.weight         torch.Size([256])
img_encoder.model.6.4.bn1.bias   torch.Size([256])
img_encoder.model.6.4.bn1.running_mean   torch.Size([256])
img_encoder.model.6.4.bn1.running_var    torch.Size([256])
img_encoder.model.6.4.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.6.4.conv2.weight       torch.Size([256, 256, 3, 3])
img_encoder.model.6.4.bn2.weight         torch.Size([256])
img_encoder.model.6.4.bn2.bias   torch.Size([256])
img_encoder.model.6.4.bn2.running_mean   torch.Size([256])
img_encoder.model.6.4.bn2.running_var    torch.Size([256])
img_encoder.model.6.4.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.6.4.conv3.weight       torch.Size([1024, 256, 1, 1])
img_encoder.model.6.4.bn3.weight         torch.Size([1024])
img_encoder.model.6.4.bn3.bias   torch.Size([1024])
img_encoder.model.6.4.bn3.running_mean   torch.Size([1024])
img_encoder.model.6.4.bn3.running_var    torch.Size([1024])
img_encoder.model.6.4.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.6.5.conv1.weight       torch.Size([256, 1024, 1, 1])
img_encoder.model.6.5.bn1.weight         torch.Size([256])
img_encoder.model.6.5.bn1.bias   torch.Size([256])
img_encoder.model.6.5.bn1.running_mean   torch.Size([256])
img_encoder.model.6.5.bn1.running_var    torch.Size([256])
img_encoder.model.6.5.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.6.5.conv2.weight       torch.Size([256, 256, 3, 3])
img_encoder.model.6.5.bn2.weight         torch.Size([256])
img_encoder.model.6.5.bn2.bias   torch.Size([256])
img_encoder.model.6.5.bn2.running_mean   torch.Size([256])
img_encoder.model.6.5.bn2.running_var    torch.Size([256])
img_encoder.model.6.5.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.6.5.conv3.weight       torch.Size([1024, 256, 1, 1])
img_encoder.model.6.5.bn3.weight         torch.Size([1024])
img_encoder.model.6.5.bn3.bias   torch.Size([1024])
img_encoder.model.6.5.bn3.running_mean   torch.Size([1024])
img_encoder.model.6.5.bn3.running_var    torch.Size([1024])
img_encoder.model.6.5.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.7.0.conv1.weight       torch.Size([512, 1024, 1, 1])
img_encoder.model.7.0.bn1.weight         torch.Size([512])
img_encoder.model.7.0.bn1.bias   torch.Size([512])
img_encoder.model.7.0.bn1.running_mean   torch.Size([512])
img_encoder.model.7.0.bn1.running_var    torch.Size([512])
img_encoder.model.7.0.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.7.0.conv2.weight       torch.Size([512, 512, 3, 3])
img_encoder.model.7.0.bn2.weight         torch.Size([512])
img_encoder.model.7.0.bn2.bias   torch.Size([512])
img_encoder.model.7.0.bn2.running_mean   torch.Size([512])
img_encoder.model.7.0.bn2.running_var    torch.Size([512])
img_encoder.model.7.0.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.7.0.conv3.weight       torch.Size([2048, 512, 1, 1])
img_encoder.model.7.0.bn3.weight         torch.Size([2048])
img_encoder.model.7.0.bn3.bias   torch.Size([2048])
img_encoder.model.7.0.bn3.running_mean   torch.Size([2048])
img_encoder.model.7.0.bn3.running_var    torch.Size([2048])
img_encoder.model.7.0.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.7.0.downsample.0.weight        torch.Size([2048, 1024, 1, 1])
img_encoder.model.7.0.downsample.1.weight        torch.Size([2048])
img_encoder.model.7.0.downsample.1.bias          torch.Size([2048])
img_encoder.model.7.0.downsample.1.running_mean          torch.Size([2048])
img_encoder.model.7.0.downsample.1.running_var   torch.Size([2048])
img_encoder.model.7.0.downsample.1.num_batches_tracked   torch.Size([])
img_encoder.model.7.1.conv1.weight       torch.Size([512, 2048, 1, 1])
img_encoder.model.7.1.bn1.weight         torch.Size([512])
img_encoder.model.7.1.bn1.bias   torch.Size([512])
img_encoder.model.7.1.bn1.running_mean   torch.Size([512])
img_encoder.model.7.1.bn1.running_var    torch.Size([512])
img_encoder.model.7.1.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.7.1.conv2.weight       torch.Size([512, 512, 3, 3])
img_encoder.model.7.1.bn2.weight         torch.Size([512])
img_encoder.model.7.1.bn2.bias   torch.Size([512])
img_encoder.model.7.1.bn2.running_mean   torch.Size([512])
img_encoder.model.7.1.bn2.running_var    torch.Size([512])
img_encoder.model.7.1.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.7.1.conv3.weight       torch.Size([2048, 512, 1, 1])
img_encoder.model.7.1.bn3.weight         torch.Size([2048])
img_encoder.model.7.1.bn3.bias   torch.Size([2048])
img_encoder.model.7.1.bn3.running_mean   torch.Size([2048])
img_encoder.model.7.1.bn3.running_var    torch.Size([2048])
img_encoder.model.7.1.bn3.num_batches_tracked    torch.Size([])
img_encoder.model.7.2.conv1.weight       torch.Size([512, 2048, 1, 1])
img_encoder.model.7.2.bn1.weight         torch.Size([512])
img_encoder.model.7.2.bn1.bias   torch.Size([512])
img_encoder.model.7.2.bn1.running_mean   torch.Size([512])
img_encoder.model.7.2.bn1.running_var    torch.Size([512])
img_encoder.model.7.2.bn1.num_batches_tracked    torch.Size([])
img_encoder.model.7.2.conv2.weight       torch.Size([512, 512, 3, 3])
img_encoder.model.7.2.bn2.weight         torch.Size([512])
img_encoder.model.7.2.bn2.bias   torch.Size([512])
img_encoder.model.7.2.bn2.running_mean   torch.Size([512])
img_encoder.model.7.2.bn2.running_var    torch.Size([512])
img_encoder.model.7.2.bn2.num_batches_tracked    torch.Size([])
img_encoder.model.7.2.conv3.weight       torch.Size([2048, 512, 1, 1])
img_encoder.model.7.2.bn3.weight         torch.Size([2048])
img_encoder.model.7.2.bn3.bias   torch.Size([2048])
img_encoder.model.7.2.bn3.running_mean   torch.Size([2048])
img_encoder.model.7.2.bn3.running_var    torch.Size([2048])
img_encoder.model.7.2.bn3.num_batches_tracked    torch.Size([])
encoder.layer.0.attention.self.query.weight      torch.Size([512, 512])
encoder.layer.0.attention.self.query.bias        torch.Size([512])
encoder.layer.0.attention.self.key.weight        torch.Size([512, 512])
encoder.layer.0.attention.self.key.bias          torch.Size([512])
encoder.layer.0.attention.self.value.weight      torch.Size([512, 512])
encoder.layer.0.attention.self.value.bias        torch.Size([512])
encoder.layer.0.attention.output.dense.weight    torch.Size([512, 512])
encoder.layer.0.attention.output.dense.bias      torch.Size([512])
encoder.layer.0.attention.output.LayerNorm.weight        torch.Size([512])
encoder.layer.0.attention.output.LayerNorm.bias          torch.Size([512])
encoder.layer.0.intermediate.dense.weight        torch.Size([2048, 512])
encoder.layer.0.intermediate.dense.bias          torch.Size([2048])
encoder.layer.0.output.dense.weight      torch.Size([512, 2048])
encoder.layer.0.output.dense.bias        torch.Size([512])
encoder.layer.0.output.LayerNorm.weight          torch.Size([512])
encoder.layer.0.output.LayerNorm.bias    torch.Size([512])
encoder.layer.1.attention.self.query.weight      torch.Size([512, 512])
encoder.layer.1.attention.self.query.bias        torch.Size([512])
encoder.layer.1.attention.self.key.weight        torch.Size([512, 512])
encoder.layer.1.attention.self.key.bias          torch.Size([512])
encoder.layer.1.attention.self.value.weight      torch.Size([512, 512])
encoder.layer.1.attention.self.value.bias        torch.Size([512])
encoder.layer.1.attention.output.dense.weight    torch.Size([512, 512])
encoder.layer.1.attention.output.dense.bias      torch.Size([512])
encoder.layer.1.attention.output.LayerNorm.weight        torch.Size([512])
encoder.layer.1.attention.output.LayerNorm.bias          torch.Size([512])
encoder.layer.1.intermediate.dense.weight        torch.Size([2048, 512])
encoder.layer.1.intermediate.dense.bias          torch.Size([2048])
encoder.layer.1.output.dense.weight      torch.Size([512, 2048])
encoder.layer.1.output.dense.bias        torch.Size([512])
encoder.layer.1.output.LayerNorm.weight          torch.Size([512])
encoder.layer.1.output.LayerNorm.bias    torch.Size([512])
encoder.layer.2.attention.self.query.weight      torch.Size([512, 512])
encoder.layer.2.attention.self.query.bias        torch.Size([512])
encoder.layer.2.attention.self.key.weight        torch.Size([512, 512])
encoder.layer.2.attention.self.key.bias          torch.Size([512])
encoder.layer.2.attention.self.value.weight      torch.Size([512, 512])
encoder.layer.2.attention.self.value.bias        torch.Size([512])
encoder.layer.2.attention.output.dense.weight    torch.Size([512, 512])
encoder.layer.2.attention.output.dense.bias      torch.Size([512])
encoder.layer.2.attention.output.LayerNorm.weight        torch.Size([512])
encoder.layer.2.attention.output.LayerNorm.bias          torch.Size([512])
encoder.layer.2.intermediate.dense.weight        torch.Size([2048, 512])
encoder.layer.2.intermediate.dense.bias          torch.Size([2048])
encoder.layer.2.output.dense.weight      torch.Size([512, 2048])
encoder.layer.2.output.dense.bias        torch.Size([512])
encoder.layer.2.output.LayerNorm.weight          torch.Size([512])
encoder.layer.2.output.LayerNorm.bias    torch.Size([512])
encoder.layer.3.attention.self.query.weight      torch.Size([512, 512])
encoder.layer.3.attention.self.query.bias        torch.Size([512])
encoder.layer.3.attention.self.key.weight        torch.Size([512, 512])
encoder.layer.3.attention.self.key.bias          torch.Size([512])
encoder.layer.3.attention.self.value.weight      torch.Size([512, 512])
encoder.layer.3.attention.self.value.bias        torch.Size([512])
encoder.layer.3.attention.output.dense.weight    torch.Size([512, 512])
encoder.layer.3.attention.output.dense.bias      torch.Size([512])
encoder.layer.3.attention.output.LayerNorm.weight        torch.Size([512])
encoder.layer.3.attention.output.LayerNorm.bias          torch.Size([512])
encoder.layer.3.intermediate.dense.weight        torch.Size([2048, 512])
encoder.layer.3.intermediate.dense.bias          torch.Size([2048])
encoder.layer.3.output.dense.weight      torch.Size([512, 2048])
encoder.layer.3.output.dense.bias        torch.Size([512])
encoder.layer.3.output.LayerNorm.weight          torch.Size([512])
encoder.layer.3.output.LayerNorm.bias    torch.Size([512])
pooler.dense.weight      torch.Size([512, 512])
pooler.dense.bias        torch.Size([512])
cls.predictions.bias     torch.Size([30522])
cls.predictions.transform.dense.weight   torch.Size([512, 512])
cls.predictions.transform.dense.bias     torch.Size([512])
cls.predictions.transform.LayerNorm.weight       torch.Size([512])
cls.predictions.transform.LayerNorm.bias         torch.Size([512])
cls.predictions.decoder.weight   torch.Size([30522, 512])